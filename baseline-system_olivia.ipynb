{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer                    \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet of the data\n",
    "Here we take a look of expert annotations data. We can see that a text is in \"agreement throughout\" doesn't always lead to its sentiment to be \"positive\". Also, we can see \"constructive\" texts mostly fall into our classifications of ERICs (that we'll be talking below) by looking at their sd_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/ydata-ynacc-v1_0/ydata-ynacc-v1_0_expert_annotations.tsv',sep='\\t')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of ERICs\n",
    "ERICs are characterized by argumentative, respectful exchanges containing persuasive, informative, and/or sympathetic comments. They tend to stay on topic with the original article and not to contain funny, mean, or sarcastic comments. We found differences between the distribution of annotations made by trained and untrained anno- tators, but high levels of agreement within each group, suggesting that crowdsourcing annotations for this task is reliable.\n",
    "\n",
    "Now, we select the columns related to ERICs and mainly look at these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>constructiveclass</th>\n",
       "      <th>sd_type</th>\n",
       "      <th>tone</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>persuasiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes..because too many houses in EU look like t...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am frankly quite SICK of the phrase \"shoved ...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Off-topic/digression</td>\n",
       "      <td>Mean</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ya, I always wonder why the conservatives are ...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Off-topic/digression</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They are also places where you are supposed no...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop trying to make sense, it only confuses pe...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Mean</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text constructiveclass  \\\n",
       "0  Yes..because too many houses in EU look like t...      Constructive   \n",
       "1  I am frankly quite SICK of the phrase \"shoved ...  Not constructive   \n",
       "2  Ya, I always wonder why the conservatives are ...  Not constructive   \n",
       "3  They are also places where you are supposed no...  Not constructive   \n",
       "4  Stop trying to make sense, it only confuses pe...  Not constructive   \n",
       "\n",
       "                          sd_type         tone sentiment  persuasiveness  \n",
       "0             Positive/respectful  Informative   neutral  Not persuasive  \n",
       "1            Off-topic/digression         Mean  negative      Persuasive  \n",
       "2            Off-topic/digression    Sarcastic   neutral  Not persuasive  \n",
       "3  Argumentative (back and forth)    Sarcastic   neutral      Persuasive  \n",
       "4  Argumentative (back and forth)         Mean  negative      Persuasive  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text','constructiveclass','sd_type','tone','sentiment','persuasiveness']]\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Informative', 'Mean', 'Sarcastic', 'Funny', 'Controversial,Mean',\n",
       "       'Mean,Sarcastic', 'Controversial', 'Sympathetic,Sarcastic',\n",
       "       'Informative,Sarcastic', 'Informative,Mean', 'Sympathetic',\n",
       "       'Controversial,Mean,Sarcastic', 'Informative,Controversial',\n",
       "       'Sarcastic,Funny', 'Controversial,Sarcastic,Funny',\n",
       "       'Mean,Sarcastic,Funny', 'Controversial,Mean,Funny',\n",
       "       'Controversial,Funny', 'Mean,Funny',\n",
       "       'Informative,Sympathetic,Funny', 'Sympathetic,Controversial,Mean',\n",
       "       'Informative,Sympathetic', 'Informative,Controversial,Mean',\n",
       "       'Controversial,Sarcastic', 'Controversial,Mean,Sarcastic,Funny',\n",
       "       'Sympathetic,Controversial', 'Sympathetic,Funny',\n",
       "       'Controversial,NA', 'Informative,Controversial,Sarcastic',\n",
       "       'Informative,NA', 'Informative,Funny', 'Sympathetic,Mean',\n",
       "       'Informative,Controversial,Funny', 'NA,Funny',\n",
       "       'Informative,Controversial,Mean,Sarcastic',\n",
       "       'Informative,Sympathetic,Controversial',\n",
       "       'Informative,Sympathetic,Mean', 'Sarcastic,NA',\n",
       "       'Informative,Mean,Sarcastic', 'Informative,Sarcastic,Funny',\n",
       "       'Informative,Sympathetic,Sarcastic',\n",
       "       'Sympathetic,Controversial,Sarcastic', 'Informative,Mean,Funny',\n",
       "       'Sympathetic,Controversial,Funny', 'Sympathetic,NA',\n",
       "       'Informative,Controversial,Sarcastic,Funny', 'Mean,NA',\n",
       "       'Controversial,Sarcastic,NA',\n",
       "       'Informative,Sympathetic,Controversial,Mean',\n",
       "       'Sympathetic,Sarcastic,Funny', 'Sympathetic,Mean,Sarcastic,Funny',\n",
       "       'Informative,Sympathetic,NA'], dtype=object)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tone'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a column of ERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ERIC']=-1 # false\n",
    "df.loc[(df['sd_type'].str.contains(\"Off-topic/digression\")==False & (df['sd_type'].str.contains('Positive')) | df['sd_type'].str.contains('Personal')) \n",
    "   & (df['persuasiveness']=='Persuasive') \n",
    "    & (df['tone'].str.contains('Informative') | df['tone'].str.contains('Controversial') | df['tone'].str.contains('Sympathetic'))\n",
    "#     & (df['sentiment']=='neutral' | df['sentiment']=='positive')\n",
    "    & (df['constructiveclass']=='Constructive'), ['ERIC']] = 1\n",
    "\n",
    "# df.loc[df['ERIC'] != 'True', ['ERIC']] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2474 entries, 8 to 17604\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   text               2474 non-null   object\n",
      " 1   constructiveclass  2474 non-null   object\n",
      " 2   sd_type            2474 non-null   object\n",
      " 3   tone               2474 non-null   object\n",
      " 4   sentiment          2474 non-null   object\n",
      " 5   persuasiveness     2474 non-null   object\n",
      " 6   ERIC               2474 non-null   int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 154.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>constructiveclass</th>\n",
       "      <th>sd_type</th>\n",
       "      <th>tone</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>ERIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I know this was probably the best thing that e...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ghrelin is produced by your fat cells. You can...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I believe they are eaten in Venezuela? It's a ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>HF and You've got to be kidding me.... Nelson ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>So Ed - 12,000 - that's still FOUR TIMES more ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text constructiveclass  \\\n",
       "8   I know this was probably the best thing that e...      Constructive   \n",
       "9   Ghrelin is produced by your fat cells. You can...      Constructive   \n",
       "30  I believe they are eaten in Venezuela? It's a ...      Constructive   \n",
       "37  HF and You've got to be kidding me.... Nelson ...      Constructive   \n",
       "39  So Ed - 12,000 - that's still FOUR TIMES more ...      Constructive   \n",
       "\n",
       "                           sd_type         tone sentiment persuasiveness  ERIC  \n",
       "8   Argumentative (back and forth)  Informative   neutral     Persuasive     1  \n",
       "9              Positive/respectful  Informative   neutral     Persuasive     1  \n",
       "30             Positive/respectful  Informative   neutral     Persuasive     1  \n",
       "37  Argumentative (back and forth)  Informative   neutral     Persuasive     1  \n",
       "39  Argumentative (back and forth)  Informative  negative     Persuasive     1  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eric = df[df['ERIC'] == 1]\n",
    "eric.info()\n",
    "eric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2474"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_eric = len(np.array(eric.index))\n",
    "n_eric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['sd_type'].str.contains(\"Off-topic/digression\") | (df['sd_type'].str.contains('Flamewar'))) \n",
    "   & (df['persuasiveness']=='Not Persuasive') \n",
    "    & (df['tone'].str.contains('Mean') | df['tone'].str.contains('Sarcastic'))\n",
    "    & (df['sentiment']=='negative')\n",
    "    & (df['constructiveclass']!='Constructive'), ['ERIC']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneric = df[df['ERIC'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3130"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_non = len(np.array(noneric.index))\n",
    "n_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15193, 10281, 14297,  6150,   569,  3548, 14991,  1510,  6629,\n",
       "       11436])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noneric_idx = np.array(noneric.index)\n",
    "chosen_noneric_idx = np.random.choice(noneric_idx, n_eric, replace=False)\n",
    "chosen_noneric_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_noneric = df.iloc[chosen_noneric_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "5604\n"
     ]
    }
   ],
   "source": [
    "chosen_data = pd.concat([eric, noneric])\n",
    "print(chosen_data.ERIC.unique())\n",
    "print(len(chosen_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_noneric_idx = np.array([i for i in noneric_idx if i not in chosen_noneric_idx])\n",
    "other_noneric = df.iloc[other_noneric_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly take a look of how well we can predict the ERIC attribute of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, df):\n",
    "    data = df[[X, y]]\n",
    "    labels = df[y].unique()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = [np.array([],dtype='str'), np.array([],dtype='str'),np.array([],dtype='str'),np.array([],dtype='str')]\n",
    "\n",
    "    for l in labels:\n",
    "        item = data.groupby(y).get_group(l)\n",
    "        a = item[X].to_numpy()\n",
    "        \n",
    "        b = item[y].to_numpy()\n",
    "        # ?? use dummies to convert the strings to matrix of features\n",
    "#         b = item[y.name].str.get_dummies(sep=\",\").to_numpy()\n",
    "        \n",
    "        # there're cases where the number of rows/entries are fewer than 4, \n",
    "        # and will cause train_test_split to generate empty values.\n",
    "        # so we sacrifice a little accuracy of our model and include those entries in both training and testing datasets.\n",
    "        # in general, it won't affect too much because the number is small.\n",
    "        if len(item) >= 4:\n",
    "            X_train_loc, X_test_loc, y_train_loc, y_test_loc = train_test_split(a, b, test_size=0.3, random_state = 400)           \n",
    "        else:\n",
    "            X_train_loc, X_test_loc, y_train_loc, y_test_loc = [a, a, b, b]\n",
    "        \n",
    "        \n",
    "        X_train = np.concatenate((X_train_loc, X_train))\n",
    "        X_test = np.concatenate((X_test_loc, X_test))\n",
    "        y_train = np.concatenate((y_train_loc, y_train))\n",
    "        y_test = np.concatenate((y_test_loc, y_test))\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     y_train = le.fit_transform(y_train)\n",
    "#     y_test = le.transform(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = [np.array([],dtype='str'), np.array([],dtype='str'),np.array([],dtype='str'),np.array([],dtype='str')]\n",
    "X_train, X_test, y_train, y_test = split_data('text', 'ERIC', chosen_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since negative sentiment texts are much more than other types of texts, we can't directly do train_test_split (because sometimes we may fail to choose from all 4 labels and resulting error in classification_report). We need to train_test_split from each type of sentiment and combine the training/test data/labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# loop over classifiers: Naive Bayes, Supported Vectors Machine, KNN\n",
    "pipe_list = []\n",
    "grid_search_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "Training_accuracy: 0.9716981132075472\n",
      "accuracy 0.6676575505350772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.64      0.95      0.76       939\n",
      "       False       0.83      0.31      0.45       743\n",
      "\n",
      "    accuracy                           0.67      1682\n",
      "   macro avg       0.73      0.63      0.61      1682\n",
      "weighted avg       0.72      0.67      0.63      1682\n",
      "\n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "Training_accuracy: 0.9724630290668026\n",
      "accuracy 0.6617122473246135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.63      0.95      0.76       939\n",
      "       False       0.83      0.30      0.44       743\n",
      "\n",
      "    accuracy                           0.66      1682\n",
      "   macro avg       0.73      0.62      0.60      1682\n",
      "weighted avg       0.72      0.66      0.62      1682\n",
      "\n",
      "<class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n",
      "Training_accuracy: 0.5586435492095869\n",
      "accuracy 0.5582639714625446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.56      1.00      0.72       939\n",
      "       False       0.00      0.00      0.00       743\n",
      "\n",
      "    accuracy                           0.56      1682\n",
      "   macro avg       0.28      0.50      0.36      1682\n",
      "weighted avg       0.31      0.56      0.40      1682\n",
      "\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "Training_accuracy: 0.9724630290668026\n",
      "accuracy 0.6617122473246135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.63      0.95      0.76       939\n",
      "       False       0.83      0.30      0.44       743\n",
      "\n",
      "    accuracy                           0.66      1682\n",
      "   macro avg       0.73      0.62      0.60      1682\n",
      "weighted avg       0.72      0.66      0.62      1682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in [MultinomialNB(alpha=1, fit_prior=True), SVC(), KNeighborsClassifier(n_neighbors=9), LogisticRegression()]:\n",
    "    pipe = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(3,3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', clf),\n",
    "                  ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pipe_list.append(pipe)\n",
    "    print(type(pipe.named_steps['clf']))\n",
    "    training_accuracy = pipe.score(X_train, y_train)\n",
    "    print('Training_accuracy:',training_accuracy)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "#     print(unique)\n",
    "#     print(counts)\n",
    "    \n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred, target_names=['True','False']))\n",
    "\n",
    "#     y_pred = pipe.predict(np.append(other_noneric.text, X_test))\n",
    "#     y_true = np.append(other_noneric.ERIC, y_test)\n",
    "\n",
    "#     print('accuracy %s' % accuracy_score(y_pred, y_true))\n",
    "#     print(classification_report(y_true, y_pred, target_names=chosen_data.ERIC.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx = np.argsort(pipe_list[0]['tfidf'].idf_)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text with the rule of Bag of Words \n",
    "def words_in_texts(words, texts):\n",
    "    '''\n",
    "    Inputs:\n",
    "        words (list-like): words to find\n",
    "        texts (Series): strings to search in\n",
    "    \n",
    "    Output:\n",
    "        NumPy array of 0s and 1s with shape (n, p) where n is the\n",
    "        number of texts and p is the number of words.\n",
    "    '''\n",
    "    nested_arr = []\n",
    "    for text in texts:\n",
    "        arr = []\n",
    "        for word in words:\n",
    "            if word in text:\n",
    "                arr.append(1)\n",
    "            else:\n",
    "                arr.append(0)\n",
    "        nested_arr.append(arr)\n",
    "    return nested_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # w/ preprocess\n",
    "some_words = ['please','thanks','suggest','advice','note']\n",
    "X_train = words_in_texts(some_words,X_train)\n",
    "X_test = words_in_texts(some_words,X_test)\n",
    "# y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "# training_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training_accuracy:',training_accuracy)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# test_accuracy = model.score(y_test,y_pred)\n",
    "\n",
    "# print('Logistic Regression accuracy:',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1                          \n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3922 samples, validate on 1682 samples\n",
      "Epoch 1/10\n",
      "3922/3922 [==============================] - 34s 9ms/step - loss: 0.5134 - accuracy: 0.7473 - val_loss: 0.4583 - val_accuracy: 0.7889\n",
      "Epoch 2/10\n",
      "3922/3922 [==============================] - 32s 8ms/step - loss: 0.3082 - accuracy: 0.8761 - val_loss: 0.4565 - val_accuracy: 0.7949\n",
      "Epoch 3/10\n",
      "3922/3922 [==============================] - 33s 8ms/step - loss: 0.1863 - accuracy: 0.9319 - val_loss: 0.5416 - val_accuracy: 0.7883\n",
      "Epoch 4/10\n",
      "3922/3922 [==============================] - 32s 8ms/step - loss: 0.1388 - accuracy: 0.9541 - val_loss: 0.5788 - val_accuracy: 0.7866\n",
      "Epoch 5/10\n",
      "3922/3922 [==============================] - 32s 8ms/step - loss: 0.1263 - accuracy: 0.9582 - val_loss: 0.5745 - val_accuracy: 0.7919\n",
      "Epoch 6/10\n",
      "3922/3922 [==============================] - 34s 9ms/step - loss: 0.1191 - accuracy: 0.9556 - val_loss: 0.6111 - val_accuracy: 0.7872\n",
      "Epoch 7/10\n",
      "3922/3922 [==============================] - 32s 8ms/step - loss: 0.1125 - accuracy: 0.9567 - val_loss: 0.6772 - val_accuracy: 0.7854\n",
      "Epoch 8/10\n",
      "3922/3922 [==============================] - 33s 8ms/step - loss: 0.1031 - accuracy: 0.9579 - val_loss: 0.6198 - val_accuracy: 0.7788\n",
      "Epoch 9/10\n",
      "3922/3922 [==============================] - 33s 8ms/step - loss: 0.0933 - accuracy: 0.9595 - val_loss: 0.7385 - val_accuracy: 0.7717\n",
      "Epoch 10/10\n",
      "3922/3922 [==============================] - 33s 8ms/step - loss: 0.0958 - accuracy: 0.9582 - val_loss: 0.6846 - val_accuracy: 0.7794\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes w/ gridSearch\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB()),\n",
    "                  ])\n",
    "params = {'vect__min_df': np.linspace(0.005, 0.05, 5),\n",
    "            'vect__ngram_range': ((1, 1),(1, 2),(2, 2)),  # unigrams or bigrams\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__alpha': np.logspace(0,1,10),\n",
    "            'clf__fit_prior': (True, False),\n",
    "            }\n",
    "search = GridSearchCV(pipe, param_grid=params)\n",
    "search.fit(X_train, y_train)\n",
    "grid_search_list.append(search)\n",
    "print(\"Best parameter values:\")\n",
    "for param in search.best_params_.items():\n",
    "    print(param)\n",
    "print(\"CV Score using best parameter values:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM w/ gridSearch\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', SVC()),\n",
    "                  ])\n",
    "params = {'vect__min_df': np.linspace(0.005, 0.05, 5),\n",
    "            'vect__ngram_range': ((1, 1),(1, 2),(2, 2)),  # unigrams or bigrams\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "            'clf__gamma': ('scale', 'auto'),\n",
    "            }\n",
    "search = GridSearchCV(pipe, param_grid=params)\n",
    "search.fit(X_train, y_train)\n",
    "grid_search_list.append(search)\n",
    "print(\"Best parameter values:\")\n",
    "for param in search.best_params_.items():\n",
    "    print(param)\n",
    "print(\"CV Score using best parameter values:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN w/ gridSearch \n",
    "\n",
    "pipe2 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', KNeighborsClassifier()),\n",
    "              ])\n",
    "# pipe2.fit(X_train, y_train)\n",
    "# print(pipe2.named_steps)\n",
    "# y_pred = pipe2.predict(X_test)\n",
    "\n",
    "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred, target_names=df.sentiment.unique()))\n",
    "\n",
    "params = {\n",
    "            'vect__min_df': np.linspace(0.005, 0.05, 5),\n",
    "            'vect__ngram_range': ((1, 1),(1, 2),(2, 2)),  # unigrams or bigrams\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__n_neighbors': (5,6,7,8,9),\n",
    "            'clf__weights': ('uniform', 'distance')\n",
    "#             'clf__fit_prior': (True, False),\n",
    "            }\n",
    "search = GridSearchCV(pipe2, param_grid=params)\n",
    "search.fit(X_train, y_train)\n",
    "grid_search_list.append(search)\n",
    "print(\"Best parameter values:\")\n",
    "for param in search.best_params_.items():\n",
    "    print(param)\n",
    "print(\"CV Score using best parameter values:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipe2.named_steps['clf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type_dum = df['sd_type'].str.get_dummies(sep=\",\")\n",
    "type_dum_arr = type_dum.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dum_name = np.array(type_dum.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for ERIC and non-ERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>constructiveclass</th>\n",
       "      <th>sd_type</th>\n",
       "      <th>tone</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>ERIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I know this was probably the best thing that e...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ghrelin is produced by your fat cells. You can...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I believe they are eaten in Venezuela? It's a ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>HF and You've got to be kidding me.... Nelson ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>So Ed - 12,000 - that's still FOUR TIMES more ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text constructiveclass  \\\n",
       "8   I know this was probably the best thing that e...      Constructive   \n",
       "9   Ghrelin is produced by your fat cells. You can...      Constructive   \n",
       "30  I believe they are eaten in Venezuela? It's a ...      Constructive   \n",
       "37  HF and You've got to be kidding me.... Nelson ...      Constructive   \n",
       "39  So Ed - 12,000 - that's still FOUR TIMES more ...      Constructive   \n",
       "\n",
       "                           sd_type         tone sentiment persuasiveness  ERIC  \n",
       "8   Argumentative (back and forth)  Informative   neutral     Persuasive     1  \n",
       "9              Positive/respectful  Informative   neutral     Persuasive     1  \n",
       "30             Positive/respectful  Informative   neutral     Persuasive     1  \n",
       "37  Argumentative (back and forth)  Informative   neutral     Persuasive     1  \n",
       "39  Argumentative (back and forth)  Informative  negative     Persuasive     1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>constructiveclass</th>\n",
       "      <th>sd_type</th>\n",
       "      <th>tone</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>ERIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes..because too many houses in EU look like t...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not persuasive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am frankly quite SICK of the phrase \"shoved ...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Off-topic/digression</td>\n",
       "      <td>Mean</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ya, I always wonder why the conservatives are ...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Off-topic/digression</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not persuasive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They are also places where you are supposed no...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop trying to make sense, it only confuses pe...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Mean</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text constructiveclass  \\\n",
       "0  Yes..because too many houses in EU look like t...      Constructive   \n",
       "1  I am frankly quite SICK of the phrase \"shoved ...  Not constructive   \n",
       "2  Ya, I always wonder why the conservatives are ...  Not constructive   \n",
       "3  They are also places where you are supposed no...  Not constructive   \n",
       "4  Stop trying to make sense, it only confuses pe...  Not constructive   \n",
       "\n",
       "                          sd_type         tone sentiment  persuasiveness  ERIC  \n",
       "0             Positive/respectful  Informative   neutral  Not persuasive     0  \n",
       "1            Off-topic/digression         Mean  negative      Persuasive     0  \n",
       "2            Off-topic/digression    Sarcastic   neutral  Not persuasive     0  \n",
       "3  Argumentative (back and forth)    Sarcastic   neutral      Persuasive     0  \n",
       "4  Argumentative (back and forth)         Mean  negative      Persuasive     0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noneric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black lives matter', 'techniques knowing world', 'difference right wrong', 'knowing world overlap', 'self defense scenario', 'step right direction', 'living paycheck paycheck', '16 year old', 'locker rooms bathrooms', 'prohibits government prohibiting', 'government prohibiting free', 'saying black people', '10 20 percent', 'paying fair share', 'young earth creationism', 'prohibiting free exercise', 'false sense security', 'free exercise religion', 'right act way', 'gay marriage supreme', 'completely different epistemological', 'hope door time', 'throwing away billions', 'liberal religionists gentle', 'door time comes', 'black people didn', 'people food stamps', 'companies time fail', 'just don believe', 'white black people', 'don want change', 'green energy companies', 'time comes ban', 'stamps cost money', 'act way people', 'won happen like', 'letting isis expand', 'live let live', 'gun violence america', 'time fail costs', 'different epistemological techniques', 'syria libya cost', 'billions dollars green', 'expand won happen', 'gay people born', 'threaten public figures', 'lot willing thing', 'people using bathroom', 'using restroom choice', 'makes feel better']\n"
     ]
    }
   ],
   "source": [
    "# extract featured phrases for ERICs\n",
    "pipe_eric = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(3,3))),\n",
    "               ('tfidf', TfidfTransformer())])\n",
    "pipe_eric.fit(eric.text)\n",
    "eric_idx = np.argsort(pipe_eric['tfidf'].idf_)[:50]\n",
    "feat = pipe_eric['vect'].get_feature_names()\n",
    "eric_feat = [feat[i] for i in eric_idx]\n",
    "print(eric_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new york city', 'world better place', 'inbred racist idiots', 'spoken like true', 'comments just continue', 'mom horrible person', 'piece dogshit god', 'black lives matter', 'independently probably destroyed', 'poor excuse human', 'makes feel better', 'probably destroyed remaining', 'remaining common sense', 'excuse human dung', 'image reality knows', 'inability think independently', 'hate inability think', 'destroyed remaining common', 'known multi account', 'think independently probably', 'feeling hate inability', 'called dominant group', 'leader free world', 'proceeds received free', 'calling don forget', 'nimrods fail ways', 'got head start', 'gotta play like', 'group got head', 'doesn suggest rape', 'dominant group got', '40 years ago', 'wall hope sticks', 'head start proceeds', 'start proceeds received', 'like typical liberals', 'let know ll', 'school improve life', 'people skin look', 'george bush lied', 'life outside employment', 'ways school improve', 'like tomato sauce', 'clowns common sense', 'received free labor', 'fail ways school', 'improve life outside', 'outside employment amuse', 'idiots like think', 'interested electing career']\n"
     ]
    }
   ],
   "source": [
    "# extract featured phrases for non-ERICs\n",
    "pipe_noneric = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(3,3))),\n",
    "               ('tfidf', TfidfTransformer())])\n",
    "pipe_noneric.fit(noneric.text)\n",
    "noneric_idx = np.argsort(pipe_noneric['tfidf'].idf_)[:50]\n",
    "feat2 = pipe_noneric['vect'].get_feature_names()\n",
    "noneric_feat = [feat2[i] for i in noneric_idx]\n",
    "print(noneric_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black lives matter', 'makes feel better']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in eric_feat if i in noneric_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
