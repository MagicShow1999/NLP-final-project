{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer                    \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet of the data\n",
    "Here we take a look of expert annotations data. We can see that a text is in \"agreement throughout\" doesn't always lead to its sentiment to be \"positive\". Also, we can see \"constructive\" texts mostly fall into our classifications of ERICs (that we'll be talking below) by looking at their sd_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/yahoodata/ydata-ynacc-v1_0_expert_annotations.tsv',sep='\\t')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of ERICs\n",
    "ERICs are characterized by argumentative, respectful exchanges containing persuasive, informative, and/or sympathetic comments. They tend to stay on topic with the original article and not to contain funny, mean, or sarcastic comments. We found differences between the distribution of annotations made by trained and untrained anno- tators, but high levels of agreement within each group, suggesting that crowdsourcing annotations for this task is reliable.\n",
    "\n",
    "Now, we select the columns related to ERICs and mainly look at these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>constructiveclass</th>\n",
       "      <th>sd_type</th>\n",
       "      <th>tone</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>persuasiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes..because too many houses in EU look like t...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am frankly quite SICK of the phrase \"shoved ...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Off-topic/digression</td>\n",
       "      <td>Mean</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ya, I always wonder why the conservatives are ...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Off-topic/digression</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They are also places where you are supposed no...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop trying to make sense, it only confuses pe...</td>\n",
       "      <td>Not constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Mean</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text constructiveclass  \\\n",
       "0  Yes..because too many houses in EU look like t...      Constructive   \n",
       "1  I am frankly quite SICK of the phrase \"shoved ...  Not constructive   \n",
       "2  Ya, I always wonder why the conservatives are ...  Not constructive   \n",
       "3  They are also places where you are supposed no...  Not constructive   \n",
       "4  Stop trying to make sense, it only confuses pe...  Not constructive   \n",
       "\n",
       "                          sd_type         tone sentiment  persuasiveness  \n",
       "0             Positive/respectful  Informative   neutral  Not persuasive  \n",
       "1            Off-topic/digression         Mean  negative      Persuasive  \n",
       "2            Off-topic/digression    Sarcastic   neutral  Not persuasive  \n",
       "3  Argumentative (back and forth)    Sarcastic   neutral      Persuasive  \n",
       "4  Argumentative (back and forth)         Mean  negative      Persuasive  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text','constructiveclass','sd_type','tone','sentiment','persuasiveness']]\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Informative', 'Mean', 'Sarcastic', 'Funny', 'Controversial,Mean',\n",
       "       'Mean,Sarcastic', 'Controversial', 'Sympathetic,Sarcastic',\n",
       "       'Informative,Sarcastic', 'Informative,Mean', 'Sympathetic',\n",
       "       'Controversial,Mean,Sarcastic', 'Informative,Controversial',\n",
       "       'Sarcastic,Funny', 'Controversial,Sarcastic,Funny',\n",
       "       'Mean,Sarcastic,Funny', 'Controversial,Mean,Funny',\n",
       "       'Controversial,Funny', 'Mean,Funny',\n",
       "       'Informative,Sympathetic,Funny', 'Sympathetic,Controversial,Mean',\n",
       "       'Informative,Sympathetic', 'Informative,Controversial,Mean',\n",
       "       'Controversial,Sarcastic', 'Controversial,Mean,Sarcastic,Funny',\n",
       "       'Sympathetic,Controversial', 'Sympathetic,Funny',\n",
       "       'Controversial,NA', 'Informative,Controversial,Sarcastic',\n",
       "       'Informative,NA', 'Informative,Funny', 'Sympathetic,Mean',\n",
       "       'Informative,Controversial,Funny', 'NA,Funny',\n",
       "       'Informative,Controversial,Mean,Sarcastic',\n",
       "       'Informative,Sympathetic,Controversial',\n",
       "       'Informative,Sympathetic,Mean', 'Sarcastic,NA',\n",
       "       'Informative,Mean,Sarcastic', 'Informative,Sarcastic,Funny',\n",
       "       'Informative,Sympathetic,Sarcastic',\n",
       "       'Sympathetic,Controversial,Sarcastic', 'Informative,Mean,Funny',\n",
       "       'Sympathetic,Controversial,Funny', 'Sympathetic,NA',\n",
       "       'Informative,Controversial,Sarcastic,Funny', 'Mean,NA',\n",
       "       'Controversial,Sarcastic,NA',\n",
       "       'Informative,Sympathetic,Controversial,Mean',\n",
       "       'Sympathetic,Sarcastic,Funny', 'Sympathetic,Mean,Sarcastic,Funny',\n",
       "       'Informative,Sympathetic,NA'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tone'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a column of ERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ERIC']=-1 # false\n",
    "df.loc[(df['sd_type'].str.contains(\"Off-topic/digression\")==False & (df['sd_type'].str.contains('Positive')) | df['sd_type'].str.contains('Personal')) \n",
    "   & (df['persuasiveness']=='Persuasive') \n",
    "    & (df['tone'].str.contains('Informative') | df['tone'].str.contains('Controversial') | df['tone'].str.contains('Sympathetic'))\n",
    "#     & (df['sentiment']=='neutral' | df['sentiment']=='positive')\n",
    "    & (df['constructiveclass']=='Constructive'), ['ERIC']] = 1\n",
    "\n",
    "# df.loc[df['ERIC'] != 'True', ['ERIC']] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2474 entries, 8 to 17604\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   text               2474 non-null   object\n",
      " 1   constructiveclass  2474 non-null   object\n",
      " 2   sd_type            2474 non-null   object\n",
      " 3   tone               2474 non-null   object\n",
      " 4   sentiment          2474 non-null   object\n",
      " 5   persuasiveness     2474 non-null   object\n",
      " 6   ERIC               2474 non-null   int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 154.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>constructiveclass</th>\n",
       "      <th>sd_type</th>\n",
       "      <th>tone</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>persuasiveness</th>\n",
       "      <th>ERIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I know this was probably the best thing that e...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ghrelin is produced by your fat cells. You can...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I believe they are eaten in Venezuela? It's a ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Positive/respectful</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>HF and You've got to be kidding me.... Nelson ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>So Ed - 12,000 - that's still FOUR TIMES more ...</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Argumentative (back and forth)</td>\n",
       "      <td>Informative</td>\n",
       "      <td>negative</td>\n",
       "      <td>Persuasive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text constructiveclass  \\\n",
       "8   I know this was probably the best thing that e...      Constructive   \n",
       "9   Ghrelin is produced by your fat cells. You can...      Constructive   \n",
       "30  I believe they are eaten in Venezuela? It's a ...      Constructive   \n",
       "37  HF and You've got to be kidding me.... Nelson ...      Constructive   \n",
       "39  So Ed - 12,000 - that's still FOUR TIMES more ...      Constructive   \n",
       "\n",
       "                           sd_type         tone sentiment persuasiveness  ERIC  \n",
       "8   Argumentative (back and forth)  Informative   neutral     Persuasive     1  \n",
       "9              Positive/respectful  Informative   neutral     Persuasive     1  \n",
       "30             Positive/respectful  Informative   neutral     Persuasive     1  \n",
       "37  Argumentative (back and forth)  Informative   neutral     Persuasive     1  \n",
       "39  Argumentative (back and forth)  Informative  negative     Persuasive     1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eric = df[df['ERIC'] == 1]\n",
    "eric.info()\n",
    "eric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2474"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_eric = len(np.array(eric.index))\n",
    "n_eric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[(df['sd_type'].str.contains(\"Off-topic/digression\") | (df['sd_type'].str.contains('Flamewar'))) \n",
    "   & (df['persuasiveness']=='Not persuasive') \n",
    "    & (df['tone'].str.contains('Mean') | df['tone'].str.contains('Sarcastic'))\n",
    "    & (df['sentiment']=='negative')\n",
    "    & (df['constructiveclass']!='Constructive'), ['ERIC']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2088"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_non = len(np.array(noneric.index))\n",
    "n_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7405,  8765, 17355, 14648,  5799,  9520,  4230, 13540,  5808,\n",
       "         463])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_eric_idx = np.random.choice(eric.index, n_non, replace=False)\n",
    "chosen_eric_idx[:10]\n",
    "\n",
    "# noneric_idx = np.array(noneric.index)\n",
    "# chosen_noneric_idx = np.random.choice(noneric_idx, n_eric, replace=False)\n",
    "# chosen_noneric_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_eric = df.iloc[chosen_eric_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "4562\n"
     ]
    }
   ],
   "source": [
    "chosen_data = pd.concat([eric, noneric])\n",
    "print(chosen_data.ERIC.unique())\n",
    "print(len(chosen_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_noneric_idx = np.array([i for i in noneric_idx if i not in chosen_eric_idx])\n",
    "other_noneric = df.iloc[other_noneric_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly take a look of how well we can predict the ERIC attribute of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, df):\n",
    "    data = df[[X, y]]\n",
    "    labels = df[y].unique()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = [np.array([],dtype='str'), np.array([],dtype='str'),np.array([],dtype='str'),np.array([],dtype='str')]\n",
    "\n",
    "    for l in labels:\n",
    "        item = data.groupby(y).get_group(l)\n",
    "        a = item[X].to_numpy()\n",
    "        \n",
    "        b = item[y].to_numpy()\n",
    "        # ?? use dummies to convert the strings to matrix of features\n",
    "#         b = item[y.name].str.get_dummies(sep=\",\").to_numpy()\n",
    "        \n",
    "        # there're cases where the number of rows/entries are fewer than 4, \n",
    "        # and will cause train_test_split to generate empty values.\n",
    "        # so we sacrifice a little accuracy of our model and include those entries in both training and testing datasets.\n",
    "        # in general, it won't affect too much because the number is small.\n",
    "        if len(item) >= 4:\n",
    "            X_train_loc, X_test_loc, y_train_loc, y_test_loc = train_test_split(a, b, test_size=0.3, random_state = 400)           \n",
    "        else:\n",
    "            X_train_loc, X_test_loc, y_train_loc, y_test_loc = [a, a, b, b]\n",
    "        \n",
    "        \n",
    "        X_train = np.concatenate((X_train_loc, X_train))\n",
    "        X_test = np.concatenate((X_test_loc, X_test))\n",
    "        y_train = np.concatenate((y_train_loc, y_train))\n",
    "        y_test = np.concatenate((y_test_loc, y_test))\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     y_train = le.fit_transform(y_train)\n",
    "#     y_test = le.transform(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = [np.array([],dtype='str'), np.array([],dtype='str'),np.array([],dtype='str'),np.array([],dtype='str')]\n",
    "X_train, X_test, y_train, y_test = split_data('text', 'ERIC', chosen_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since negative sentiment texts are much more than other types of texts, we can't directly do train_test_split (because sometimes we may fail to choose from all 4 labels and resulting error in classification_report). We need to train_test_split from each type of sentiment and combine the training/test data/labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# loop over classifiers: Naive Bayes, Supported Vectors Machine, KNN\n",
    "pipe_list = []\n",
    "grid_search_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "Training_accuracy: 0.9489348370927319\n",
      "accuracy 0.6817518248175183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.87      0.36      0.51       627\n",
      "       False       0.64      0.96      0.77       743\n",
      "\n",
      "    accuracy                           0.68      1370\n",
      "   macro avg       0.75      0.66      0.64      1370\n",
      "weighted avg       0.74      0.68      0.65      1370\n",
      "\n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "Training_accuracy: 0.9899749373433584\n",
      "accuracy 0.6605839416058394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.58      0.93      0.72       627\n",
      "       False       0.88      0.43      0.58       743\n",
      "\n",
      "    accuracy                           0.66      1370\n",
      "   macro avg       0.73      0.68      0.65      1370\n",
      "weighted avg       0.74      0.66      0.64      1370\n",
      "\n",
      "<class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n",
      "Training_accuracy: 0.45770676691729323\n",
      "accuracy 0.4576642335766423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.46      1.00      0.63       627\n",
      "       False       0.00      0.00      0.00       743\n",
      "\n",
      "    accuracy                           0.46      1370\n",
      "   macro avg       0.23      0.50      0.31      1370\n",
      "weighted avg       0.21      0.46      0.29      1370\n",
      "\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "Training_accuracy: 0.9505012531328321\n",
      "accuracy 0.6897810218978102\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.94      0.35      0.51       627\n",
      "       False       0.64      0.98      0.77       743\n",
      "\n",
      "    accuracy                           0.69      1370\n",
      "   macro avg       0.79      0.66      0.64      1370\n",
      "weighted avg       0.78      0.69      0.65      1370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in [MultinomialNB(alpha=1, fit_prior=True),SVC(probability=True), KNeighborsClassifier(n_neighbors=9) , LogisticRegression()]:\n",
    "    pipe = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(3,3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', clf),\n",
    "                  ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pipe_list.append(pipe)\n",
    "    print(type(pipe.named_steps['clf']))\n",
    "    training_accuracy = pipe.score(X_train, y_train)\n",
    "    print('Training_accuracy:',training_accuracy)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "#     print(unique)\n",
    "#     print(counts)\n",
    "    \n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred, target_names=['True','False']))\n",
    "   \n",
    "#   y_pred = pipe.predict(np.append(other_noneric.text, X_test))\n",
    "#     y_true = np.append(other_noneric.ERIC, y_test)\n",
    "\n",
    "#     print('accuracy %s' % accuracy_score(y_pred, y_true))\n",
    "#     print(classification_report(y_true, y_pred, target_names=chosen_data.ERIC.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.svm._classes.SVC'>\n",
      "Training_accuracy: 0.9899749373433584\n",
      "accuracy 0.6605839416058394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.58      0.93      0.72       627\n",
      "       False       0.88      0.43      0.58       743\n",
      "\n",
      "    accuracy                           0.66      1370\n",
      "   macro avg       0.73      0.68      0.65      1370\n",
      "weighted avg       0.74      0.66      0.64      1370\n",
      "\n",
      "SVM system confidence score for each X_test:  [[0.93556087 0.06443913]\n",
      " [0.98459753 0.01540247]\n",
      " [0.98459753 0.01540247]\n",
      " ...\n",
      " [0.98459753 0.01540247]\n",
      " [0.98459753 0.01540247]\n",
      " [0.98459753 0.01540247]]\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(probability=True)\n",
    "pipe = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(3,3))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', clf),\n",
    "              ])\n",
    "pipe.fit(X_train, y_train)\n",
    "print(type(pipe.named_steps['clf']))\n",
    "training_accuracy = pipe.score(X_train, y_train)\n",
    "print('Training_accuracy:',training_accuracy)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "#     print(unique)\n",
    "#     print(counts)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=['True','False']))        \n",
    "class_probabilities = pipe.predict_proba(X_test)\n",
    "print('SVM system confidence score for each X_test: ', class_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93556087 0.06443913]\n",
      "#$%$ you Ed Boling. I #$%$ on you and all racists. BRING IT #$%$!!\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# an example of probability score on one comment\n",
    "print(class_probabilities[0])\n",
    "print(X_test[0])\n",
    "print(y_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text with the rule of Bag of Words \n",
    "def words_in_texts(words, texts):\n",
    "    '''\n",
    "    Inputs:\n",
    "        words (list-like): words to find\n",
    "        texts (Series): strings to search in\n",
    "    \n",
    "    Output:\n",
    "        NumPy array of 0s and 1s with shape (n, p) where n is the\n",
    "        number of texts and p is the number of words.\n",
    "    '''\n",
    "    nested_arr = []\n",
    "    for text in texts:\n",
    "        arr = []\n",
    "        for word in words:\n",
    "            if word in text:\n",
    "                arr.append(1)\n",
    "            else:\n",
    "                arr.append(0)\n",
    "        nested_arr.append(arr)\n",
    "    return nested_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # w/ preprocess\n",
    "some_words = ['please','thanks','suggest','advice','note']\n",
    "X_train = words_in_texts(some_words,X_train)\n",
    "X_test = words_in_texts(some_words,X_test)\n",
    "# y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "# training_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training_accuracy:',training_accuracy)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# test_accuracy = model.score(y_test,y_pred)\n",
    "\n",
    "# print('Logistic Regression accuracy:',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1                          \n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes w/ gridSearch\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB()),\n",
    "                  ])\n",
    "params = {'vect__min_df': np.linspace(0.005, 0.05, 5),\n",
    "            'vect__ngram_range': ((1, 1),(1, 2),(2, 2)),  # unigrams or bigrams\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__alpha': np.logspace(0,1,10),\n",
    "            'clf__fit_prior': (True, False),\n",
    "            }\n",
    "search = GridSearchCV(pipe, param_grid=params)\n",
    "search.fit(X_train, y_train)\n",
    "grid_search_list.append(search)\n",
    "print(\"Best parameter values:\")\n",
    "for param in search.best_params_.items():\n",
    "    print(param)\n",
    "print(\"CV Score using best parameter values:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM w/ gridSearch\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', SVC()),\n",
    "                  ])\n",
    "params = {'vect__min_df': np.linspace(0.005, 0.05, 5),\n",
    "            'vect__ngram_range': ((1, 1),(1, 2),(2, 2)),  # unigrams or bigrams\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "            'clf__gamma': ('scale', 'auto'),\n",
    "            }\n",
    "search = GridSearchCV(pipe, param_grid=params)\n",
    "search.fit(X_train, y_train)\n",
    "grid_search_list.append(search)\n",
    "print(\"Best parameter values:\")\n",
    "for param in search.best_params_.items():\n",
    "    print(param)\n",
    "print(\"CV Score using best parameter values:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN w/ gridSearch \n",
    "\n",
    "pipe2 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', KNeighborsClassifier()),\n",
    "              ])\n",
    "# pipe2.fit(X_train, y_train)\n",
    "# print(pipe2.named_steps)\n",
    "# y_pred = pipe2.predict(X_test)\n",
    "\n",
    "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred, target_names=df.sentiment.unique()))\n",
    "\n",
    "params = {\n",
    "            'vect__min_df': np.linspace(0.005, 0.05, 5),\n",
    "            'vect__ngram_range': ((1, 1),(1, 2),(2, 2)),  # unigrams or bigrams\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__n_neighbors': (5,6,7,8,9),\n",
    "            'clf__weights': ('uniform', 'distance')\n",
    "#             'clf__fit_prior': (True, False),\n",
    "            }\n",
    "search = GridSearchCV(pipe2, param_grid=params)\n",
    "search.fit(X_train, y_train)\n",
    "grid_search_list.append(search)\n",
    "print(\"Best parameter values:\")\n",
    "for param in search.best_params_.items():\n",
    "    print(param)\n",
    "print(\"CV Score using best parameter values:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipe2.named_steps['clf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type_dum = df['sd_type'].str.get_dummies(sep=\",\")\n",
    "type_dum_arr = type_dum.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dum_name = np.array(type_dum.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for ERIC and non-ERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract featured phrases for ERICs\n",
    "pipe_eric = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(3,3))),\n",
    "               ('tfidf', TfidfTransformer())])\n",
    "pipe_eric.fit(eric.text)\n",
    "eric_idx = np.argsort(pipe_eric['tfidf'].idf_)[:50]\n",
    "feat = pipe_eric['vect'].get_feature_names()\n",
    "eric_feat = [feat[i] for i in eric_idx]\n",
    "print(eric_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract featured phrases for non-ERICs\n",
    "pipe_noneric = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(3,3))),\n",
    "               ('tfidf', TfidfTransformer())])\n",
    "pipe_noneric.fit(noneric.text)\n",
    "noneric_idx = np.argsort(pipe_noneric['tfidf'].idf_)[:50]\n",
    "feat2 = pipe_noneric['vect'].get_feature_names()\n",
    "noneric_feat = [feat2[i] for i in noneric_idx]\n",
    "print(noneric_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in eric_feat if i in noneric_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
